{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c508b-e776-4bfb-93b2-092f4a8e3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data clustering\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, n_clusters, data,use_kmeans=True,c1=2.05, c2=2.05): # constructor\n",
    "        \n",
    "        col_max=list(data.max(axis=0))\n",
    "        col_min=list(data.min(axis=0))\n",
    "        bounds=np.column_stack([col_max,col_min])\n",
    "        self.ocentroids_pos=np.zeros((3,4))                                  #cluster, feature\n",
    "        self.centroids_pos=np.zeros((3,4))                                   #cluster, feature\n",
    "        self.n_clusters = n_clusters\n",
    "        if use_kmeans == True:\n",
    "            k_means = KMeans(n_clusters=self.n_clusters)\n",
    "            k_means.fit(data)\n",
    "            self.centroids_pos = k_means.cluster_centers_\n",
    "                 \n",
    "        else:\n",
    "            samples = np.zeros((3,4))                                       # cluster, feature\n",
    "            for i in range(3):                                              #no of cluster\n",
    "                R=np.random.uniform(0,1)\n",
    "                for j in range(4):                                           # no of feature\n",
    "                    R=np.random.uniform(0,1)\n",
    "                    s1 = bounds[j,1]+R*(bounds[j,0]-bounds[j,1])\n",
    "                    samples[i,j]=s1\n",
    "                    self.centroids_pos=samples\n",
    "        \n",
    "        \n",
    "            \n",
    "            osamples=np.zeros((3,4))                                       #cluster, feature\n",
    "            for i in range(3):                                             #Q-opposition based learning method  # no of cluster\n",
    "                for j in range(4):                                          #no of features\n",
    "                    M=((bounds[j,0]+bounds[j,1])/2)\n",
    "                    N=M*2\n",
    "                    if(self.centroids_pos[i,j]<M):\n",
    "                        s2=M+((N-self.centroids_pos[i,j])-M)*np.random.uniform(0,1)\n",
    "                        osamples[i,j]=s2\n",
    "                        self.ocentroids_pos=osamples\n",
    "                    else:\n",
    "                        a=N-self.centroids_pos[i,j]\n",
    "                        b=N-self.centroids_pos[i,j]\n",
    "                        s2=a+(M-b)*np.random.uniform(0,1)                               \n",
    "                        osamples[i,j]=s2\n",
    "                        self.ocentroids_pos=osamples\n",
    "                                   \n",
    "        \n",
    "        self.pb_val = np.inf\n",
    "        self.pb_val = np.inf\n",
    "        self.pb_pos = self.centroids_pos.copy() \n",
    "        self.velocity = np.zeros_like(self.centroids_pos) \n",
    "        # best data clustering so far\n",
    "        self.pb_clustering = None\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.fk = 0.01\n",
    "            \n",
    "    def update_pb(self, data: np.ndarray):                       \n",
    "         distances = self._get_distances(data=data)\n",
    "         clusters = np.argmin(distances, axis=0)  \n",
    "         clusters_ids = np.unique(clusters)   \n",
    "         new_val=self.withinclustersumofsquarefitness_function(clusters=clusters, distances=distances)\n",
    "         if new_val < self.pb_val:\n",
    "            self.pb_val = new_val\n",
    "            self.pb_pos = self.centroids_pos.copy()\n",
    "            self.pb_clustering = clusters.copy()\n",
    "                  \n",
    "    \n",
    "    def nupdate_pb(self, data: np.ndarray):                       \n",
    "         distances = self._get_distances(data=data)\n",
    "         clusters = np.argmin(distances, axis=0)  \n",
    "         clusters_ids = np.unique(clusters)\n",
    "         new_val=self.withinclustersumofsquarefitness_function(clusters=clusters, distances=distances)\n",
    "         return(new_val)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def oupdate_pb(self, data: np.ndarray):                      \n",
    "         distances = self._oget_distances(data=data)\n",
    "         clusters = np.argmin(distances, axis=0)  \n",
    "         clusters_ids = np.unique(clusters)\n",
    "         new_val=self.withinclustersumofsquarefitness_function(clusters=clusters, distances=distances)\n",
    "         return(new_val)\n",
    "         \n",
    "                                 \n",
    "            \n",
    "            \n",
    "    def update_velocity(self, df, max_itr, cur_itr, gb_pos: np.ndarray):\n",
    "        \"\"\"\n",
    "        Updates new velocity based on the current velocity, personal best position so far, and the swarm (global) best\n",
    "        position so far.\n",
    "        :param gb_pos: vector of best centroid positions among all particles so far\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print()\n",
    "        self.w=0.9\n",
    "        self.w=0.9-(((0.9-0.2)/max_itr)*cur_itr)\n",
    "        #self.w = 0.2 + (((0.9 - 0.2) / max_itr) * cur_itr)\n",
    "        self.fk=4*self.fk*(1-self.fk)\n",
    "        self.w=self.w*self.fk\n",
    "        self.velocity = self.w * self.velocity + \\\n",
    "                        self.c1 * np.random.random() * (self.pb_pos - self.centroids_pos) + \\\n",
    "                        self.c2 * np.random.random() * (gb_pos - self.centroids_pos)\n",
    "        phi=self.c1+self.c2\n",
    "        cfk=2/abs(2-phi-math.sqrt(math.pow(phi,2)-4*phi))\n",
    "        self.velocity=cfk*self.velocity\n",
    "        col_max=list(df.max(axis=0))\n",
    "        col_min=list(df.min(axis=0))\n",
    "        bounds=np.column_stack([col_max,col_min])\n",
    "        #print(\"bounds are:\\n\",bounds)\n",
    "        col_index=self.velocity.shape[1]\n",
    "        l_max=[]\n",
    "        l_min=[]\n",
    "        for i in range(col_index):\n",
    "            b=bounds[i,:]\n",
    "            v_max=(b[0]-b[1])/15                                                 # velocity range\n",
    "            v_min=-v_max\n",
    "            l_max.append(v_max)\n",
    "            #print(\"v max:\", v_max)\n",
    "            #print(\"v min:\",v_min)\n",
    "            l_min=[-i for i in l_max]\n",
    "        bounds=np.column_stack([l_min,l_max])\n",
    "        col_index=self.velocity.shape[1]\n",
    "        for i in range(col_index):\n",
    "            b=bounds[i,:]\n",
    "            a1=self.velocity[:,i]<b[0]\n",
    "            for j in range(a1.shape[0]):\n",
    "                if a1[j]:\n",
    "                    self.velocity[j,i]=b[0]\n",
    "            a2=self.velocity[:,i]>b[1]\n",
    "            for k in range(a2.shape[0]):\n",
    "                if a2[k]:\n",
    "                    self.velocity[k,i]=b[1]\n",
    "        \n",
    "             \n",
    "    def move_centroids(self, gb_pos,df: np.ndarray, max_itr, cur_itr):  \n",
    "              \n",
    "        self.update_velocity(df,max_itr, cur_itr,gb_pos=gb_pos)  #made changes\n",
    "        new_pos = self.centroids_pos + self.velocity \n",
    "        col_max=list(df.max(axis=0))\n",
    "        col_min=list(df.min(axis=0))\n",
    "        bounds=np.column_stack([col_min,col_max])\n",
    "        col_index=new_pos.shape[1]\n",
    "        #print(\"new pos shape 1:\", col_index)\n",
    "        for i in range(col_index):\n",
    "            b=bounds[i,:]\n",
    "            a1=new_pos[:,i]<b[0]\n",
    "            for j in range(a1.shape[0]):\n",
    "                if a1[j]:\n",
    "                    new_pos[j,i]=b[0]\n",
    "            a2=new_pos[:,i]>b[1]\n",
    "            for k in range(a2.shape[0]):\n",
    "                if a2[k]:\n",
    "                    new_pos[k,i]=b[1]\n",
    "        \n",
    "        \n",
    "        self.centroids_pos = new_pos.copy()\n",
    "        \n",
    "                    \n",
    "            \n",
    "            \n",
    "    def _get_distances(self, data: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        distances = []\n",
    "        for centroid in self.centroids_pos:\n",
    "            d = np.linalg.norm(data-centroid, axis=1)  \n",
    "            distances.append(d)\n",
    "        distances = np.array(distances)\n",
    "        return distances\n",
    "    \n",
    "    def _oget_distances(self, data: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        \n",
    "        distances = []\n",
    "        \n",
    "        for centroid in self.ocentroids_pos:\n",
    "            \n",
    "            d = np.linalg.norm(data-centroid, axis=1)  \n",
    "            distances.append(d)\n",
    "        distances = np.array(distances)\n",
    "        return distances\n",
    "    \n",
    "    \n",
    "    def _fitness_function(self, clusters: np.ndarray, distances: np.ndarray) -> float:\n",
    "        \n",
    "        J = 0.0\n",
    "        for i in range(self.n_clusters):\n",
    "            p = np.where(clusters == i)[0]  \n",
    "            if len(p):\n",
    "                d = sum(distances[i][p])\n",
    "                #print(\"d is\",d)\n",
    "                d /= len(p)\n",
    "                J += d\n",
    "        J /= self.n_clusters\n",
    "        print(\"Quantization value:\",J)\n",
    "        return J\n",
    "        \n",
    "    def withinclustersumofsquarefitness_function(self, clusters: np.ndarray, distances: np.ndarray) -> float:\n",
    "        \n",
    "        J = 0.0\n",
    "        for i in range(self.n_clusters):\n",
    "            p = np.where(clusters == i)[0] \n",
    "            if len(p):\n",
    "                #d = sum(distances[i][p]**2)\n",
    "                d = sum(distances[i][p])\n",
    "                J += d\n",
    "                \n",
    "        return J\n",
    "    \n",
    "\n",
    "    def davis(self,data:np.ndarray,clusters)-> float:\n",
    "        davis=davies_bouldin_score(data_points, clusters)\n",
    "        return davis\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ea3ec-ea29-4cc9-9b1a-8467a0376524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PSOClusteringSwarm:\n",
    "    def __init__(self, n_clusters: int, n_particles: int, kcentroids:np.ndarray,data: np.ndarray, hybrid=True,c1=2.05, c2=2.05): \n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_particles = n_particles\n",
    "        self.data = data\n",
    "        self.normal_fitness=[]\n",
    "\n",
    "        self.particles = []\n",
    "        self.newparticles=[]\n",
    "        self.newparticleso=[]\n",
    "        self.combineparticles=[] \n",
    "        self.gb_pos = None  \n",
    "        self.gb_val = np.inf \n",
    "        self.gb_clustering = None\n",
    "        self.gb_each_itr=[]\n",
    "        self._generate_particles(hybrid,c1, c2) #method call\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _print_initial(self, iteration, plot):\n",
    "        print('*** Initialing swarm with', self.n_particles, 'PARTICLES, ', self.n_clusters, 'CLUSTERS with', iteration,\n",
    "              'MAX ITERATIONS and with PLOT =', plot, '***')\n",
    "        print('Data=', self.data.shape[0], 'points in', self.data.shape[1], 'dimensions')\n",
    "\n",
    "       \n",
    "\n",
    "    def _generate_particles(self, hybrid: bool, c1: float, c2: float):  # hybrid is true\n",
    "        \"\"\"\n",
    "        Generates particles with k clusters and t-dimensional points\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for i in range(self.n_particles):\n",
    "            if i == 0 and hybrid:                                                 #\n",
    "                particle = Particle(n_clusters=self.n_clusters, data=self.data, use_kmeans=hybrid, c1=c1, c2=c2)# con. call\n",
    "                self.particles.append(particle)\n",
    "            \n",
    "            else:\n",
    "                particle = Particle(n_clusters=self.n_clusters, data=self.data, use_kmeans=False, c1=c1, c2=c2)\n",
    "                self.particles.append(particle)\n",
    "            \n",
    "        for particle in self.particles:\n",
    "            v1=particle.nupdate_pb(data=self.data)\n",
    "            self.normal_fitness.append(v1)\n",
    "            v2=particle.oupdate_pb(data=self.data)                \n",
    "            self.normal_fitness.append(v2) \n",
    "            print()\n",
    "        self.sorted_fitness=sorted(self.normal_fitness)                                   # sorted fitness\n",
    "        \n",
    "    \n",
    "        for j in range(len(self.particles)):\n",
    "            self.newparticles.append(self.particles[j].centroids_pos)\n",
    "            self.newparticleso.append(self.particles[j].ocentroids_pos)\n",
    "                \n",
    "\n",
    "\n",
    "        ###############################################################################################\n",
    "        \n",
    "        for i in range(0,30): \n",
    "            \n",
    "                self.combineparticles.append(self.particles[i].centroids_pos)\n",
    "            \n",
    "                self.combineparticles.append(self.particles[i].ocentroids_pos)\n",
    "                \n",
    "        \n",
    "        for particle in self.particles:\n",
    "            for i in range(0,30):   \n",
    "                fitness=self.sorted_fitness[i]\n",
    "                for j in range(0,60):    \n",
    "                    if self.normal_fitness[j]==fitness:\n",
    "                        self.particles[i].centroids_pos=self.combineparticles[j]\n",
    "        \n",
    "        \n",
    "     ####################################################################################################################   \n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "        for k in range(self.n_particles):\n",
    "            print()\n",
    "                 \n",
    "    \n",
    "    \n",
    "    def update_gb(self, particle):\n",
    "        if particle.pb_val < self.gb_val:\n",
    "            self.gb_val = particle.pb_val\n",
    "            self.gb_pos = particle.pb_pos.copy()\n",
    "            self.gb_clustering = particle.pb_clustering.copy()\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def start(self, iteration,data: np.ndarray,plot=False) -> Tuple[np.ndarray, float]:                         \n",
    "        z=1\n",
    "        print(\"iteration value\", iteration)\n",
    "        if (z==1):\n",
    "                for particle in self.particles:\n",
    "                    #kmeans = KMeans(n_clusters=3, init=self.particles[1].centroids_pos)\n",
    "                    #kmeans.fit(data)\n",
    "                    #self.particles[1].centroids_pos = k_means.cluster_centers_\n",
    "                    #print(\"self.particles[1].centroids_pos:\",self.particles[1].centroids_pos)\n",
    "                   \n",
    "                    #kmeans = KMeans(n_clusters=10, init=self.particles[2].centroids_pos)\n",
    "                    #kmeans.fit(data)\n",
    "                    #self.particles[2].centroids_pos = k_means.cluster_centers_\n",
    "                    \n",
    "                    #kmeans = KMeans(n_clusters=10, init=self.particles[3].centroids_pos)\n",
    "                    #kmeans.fit(data)\n",
    "                    #self.particles[3].centroids_pos = k_means.cluster_centers_\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    z=z+1\n",
    "        \n",
    "                    \n",
    "        self._print_initial(iteration, plot)\n",
    "        progress = []\n",
    "        gb_val1=0\n",
    "        temp=0\n",
    "        diff=math.inf\n",
    "        for i in range(iteration):\n",
    "            if i % 200 == 0:\n",
    "                clusters = self.gb_clustering   # none assign to clusters\n",
    "                              \n",
    "            for particle in self.particles:\n",
    "                particle.update_pb(data=self.data)\n",
    "                self.update_gb(particle=particle)\n",
    "                \n",
    "                                        \n",
    "            \n",
    "            for particle in self.particles:\n",
    "                particle.move_centroids(gb_pos=self.gb_pos, df=data, max_itr=iteration, cur_itr=i+1) \n",
    "                diff = np.abs(gb_val1 - self.gb_val)\n",
    "            gb_val1=self.gb_val\n",
    "            \n",
    "            progress.append([self.gb_pos, self.gb_clustering, self.gb_val])\n",
    "            self.gb_each_itr.append(self.gb_val)\n",
    "        print('Finished!')\n",
    "        return self.gb_clustering, self.gb_val,self.gb_pos\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c681c-c6c9-4f9d-914b-4d4b250e6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "plot=True\n",
    "data_points = pd.read_csv('iris.txt',sep=',',header=None)\n",
    "data_points = data_points[[0,1,2,3]]\n",
    "#data_points=data_points.dropna(subset=[6])                    #for cancer data set missing value\n",
    "#data_points=data_points.dropna(subset=[33])                    # for dermatology           \n",
    "data_points = data_points.values\n",
    "#k_means = KMeans(n_clusters=3)\n",
    "#k_means.fit(data_points)\n",
    "#kcentroids = k_means.cluster_centers_\n",
    "\n",
    "pso = PSOClusteringSwarm(n_clusters=3, n_particles=30, kcentroids = k_means.cluster_centers_,data=data_points,hybrid=False) \n",
    "gb_clustering, gb_val,gb_pos=pso.start(iteration=200, data=data_points,plot=plot)   \n",
    "#print(\"\\nGlobal best clustering\",gb_clustering)\n",
    "print(\"\\nGlobal best value:\", gb_val)\n",
    "print(\"\\nGlobal best position\", gb_pos)\n",
    "silhouette_avg = silhouette_score(data_points, gb_clustering)\n",
    "print(\"Sillouete score:\",silhouette_avg)\n",
    "davies_bouldin=davies_bouldin_score(data_points, gb_clustering)\n",
    "print(\"davis score:\",davies_bouldin)\n",
    "print(\"\\nGlobal best position\", gb_clustering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
